<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Talking Avatar Service Demo</title>
    <link href="./css/styles.css" rel="stylesheet">
    <!-- Azure Speech SDK (for Talking Avatar) -->
    <script src="https://aka.ms/csspeech/jsbrowserpackageraw" defer></script>
    <!-- Include any additional JS libraries if needed -->
</head>
<body>
<h1>Talking Avatar Service Demo</h1>

<div id="configuration">
  <h2 style="background-color: white; width: 300px;">Azure Speech Resource</h2>
  <label style="font-size: medium;" for="region">Region:</label>
  <select id="region" style="font-size: medium;">
    <option value="westus2">West US 2</option>
    <option value="westeurope">West Europe</option>
    <option value="southeastasia">Southeast Asia</option>
  </select>
  <label style="font-size: medium;" for="subscriptionKey">Subscription Key:</label>
  <input id="subscriptionKey" type="password" size="32" style="font-size: medium;" placeholder="Your subscription key" />
  <div style="background-color: white; width: 300px;">
    <input type="checkbox" id="enablePrivateEndpoint" onchange="window.updatePrivateEndpoint()">Enable Private Endpoint</input><br />
  </div>
  <div id="showPrivateEndpointCheckBox" hidden="hidden">
    <label style="font-size: medium;" for="privateEndpoint">Private Endpoint:</label>
    <input id="privateEndpoint" type="text" size="64" style="font-size: medium;" placeholder="https://{your custom name}.cognitiveservices.azure.com/"></input><br />
  </div>
  <br />

  <h2 style="background-color: white; width: 300px;">TTS Configuration</h2>
  <label style="font-size: medium;" for="ttsVoice">TTS Voice:</label>
  <input id="ttsVoice" type="text" size="32" style="font-size: medium;" value="en-US-AvaMultilingualNeural"></input><br />
  <label style="font-size: medium;" for="customVoiceEndpointId">Custom Voice Deployment ID (Endpoint ID):</label>
  <input id="customVoiceEndpointId" type="text" size="32" style="font-size: medium;" value=""></input><br />
  <label style="font-size: medium;" for="personalVoiceSpeakerProfileID">Personal Voice Speaker Profile ID:</label>
  <input id="personalVoiceSpeakerProfileID" type="text" size="32" style="font-size: medium;" value=""></input><br />
  <br />

  <h2 style="background-color: white; width: 300px;">Avatar Configuration</h2>
  <label style="font-size: medium;" for="talkingAvatarCharacter">Avatar Character:</label>
  <input id="talkingAvatarCharacter" type="text" size="16" style="font-size: medium;" value="lisa"></input><br />
  <label style="font-size: medium;" for="talkingAvatarStyle">Avatar Style:</label>
  <input id="talkingAvatarStyle" type="text" size="16" style="font-size: medium;" value="casual-sitting"></input><br />
  <label style="font-size: medium;" for="backgroundColor">Background Color:</label>
  <input id="backgroundColor" type="text" size="16" style="font-size: medium;" value="#FFFFFFFF"></input><br />
  <div style="background-color: white; width: 200px;">
    <input type="checkbox" id="customizedAvatar">Custom Avatar</input><br />
    <input type="checkbox" id="transparentBackground" onchange="window.updataTransparentBackground()">Transparent Background</input><br />
    <input type="checkbox" id="videoCrop">Video Crop</input><br />
  </div>
  <br />
</div>

<h2 style="background-color: white; width: 300px;">Avatar Control Panel</h2>
<label style="font-size: medium;" for="spokenText">Spoken Text:</label><br/>
<textarea id="spokenText" style="height:40px; width:90%;">Hello world!</textarea><br/>
<button id="startSession" onclick="window.startSession()">Start Session</button>
<button id="speak" onclick="window.speak()" disabled="disabled">Speak</button>
<button id="stopSpeaking" onclick="window.stopSpeaking()" disabled="disabled">Stop Speaking</button>
<button id="stopSession" onclick="window.stopSession()" disabled="disabled">Stop Session</button>
<br/>

<h2 id="videoLabel" style="background-color: white; width: 300px;">Avatar Video</h2>
<div id="videoContainer" style="position: relative; width: 960px;">
  <div id="overlayArea" style="position: absolute;" hidden="hidden">
    <p id="overlayText" style="font-size: large;">Live Video</p>
  </div>
  <div id="remoteVideo"></div>
  <canvas id="canvas" width="1920" height="1080" style="background-color: transparent;" hidden="hidden"></canvas>
  <canvas id="tmpCanvas" width="1920" height="1080" hidden="hidden"></canvas>
</div>
<br/>

<h2 style="background-color: white; width: 300px;">Logs</h2>
<div id="logging" style="background-color: white;"></div>

<script>
// Global objects for avatar synthesizer and WebRTC
var avatarSynthesizer;
var peerConnection = null; // Ensure this is declared globally
var previousAnimationFrameTimestamp = 0;

// Logger function
const log = msg => {
    document.getElementById('logging').innerHTML += msg + '<br>';
}

// Utility: HTML encoding
function htmlEncode(text) {
    const entityMap = {
        '&': '&amp;',
        '<': '&lt;',
        '>': '&gt;',
        '"': '&quot;',
        "'": '&#39;',
        '/': '&#x2F;'
    };
    return String(text).replace(/[&<>"'\/]/g, (match) => entityMap[match]);
}

// WebRTC Setup (basic implementation)
function setupWebRTC(iceServerUrl, iceServerUsername, iceServerCredential) {
    peerConnection = new RTCPeerConnection({
        iceServers: [{
            urls: [iceServerUrl],
            username: iceServerUsername,
            credential: iceServerCredential
        }]
    });
    // Handle remote video track (if applicable)
    peerConnection.ontrack = function (event) {
        let remoteVideoDiv = document.getElementById('remoteVideo');
        // Remove existing video element of the same kind
        for (var i = 0; i < remoteVideoDiv.childNodes.length; i++) {
            if (remoteVideoDiv.childNodes[i].localName === event.track.kind) {
                remoteVideoDiv.removeChild(remoteVideoDiv.childNodes[i]);
            }
        }
        const mediaPlayer = document.createElement(event.track.kind);
        mediaPlayer.id = event.track.kind;
        mediaPlayer.srcObject = event.streams[0];
        mediaPlayer.autoplay = true;
        remoteVideoDiv.appendChild(mediaPlayer);
        document.getElementById('videoLabel').hidden = true;
        document.getElementById('overlayArea').hidden = false;
    };

    peerConnection.oniceconnectionstatechange = e => {
        log("WebRTC status: " + peerConnection.iceConnectionState);
        if (peerConnection.iceConnectionState === 'connected') {
            document.getElementById('stopSession').disabled = false;
            document.getElementById('speak').disabled = false;
            document.getElementById('configuration').hidden = true;
        }
        if (peerConnection.iceConnectionState === 'disconnected' || peerConnection.iceConnectionState === 'failed') {
            document.getElementById('speak').disabled = true;
            document.getElementById('stopSpeaking').disabled = true;
            document.getElementById('stopSession').disabled = true;
            document.getElementById('startSession').disabled = false;
            document.getElementById('configuration').hidden = false;
        }
    };

    // Add transceivers for audio and video
    peerConnection.addTransceiver('video', { direction: 'sendrecv' });
    peerConnection.addTransceiver('audio', { direction: 'sendrecv' });
}

// Updated startSession: Create peerConnection if null, then start avatar
window.startSession = () => {
    const cogSvcRegion = document.getElementById('region').value;
    const cogSvcSubKey = document.getElementById('subscriptionKey').value;
    if (cogSvcSubKey === '') {
        alert('Please fill in the subscription key of your speech resource.');
        return;
    }

    const privateEndpointEnabled = document.getElementById('enablePrivateEndpoint').checked;
    const privateEndpoint = document.getElementById('privateEndpoint').value.slice(8);
    if (privateEndpointEnabled && privateEndpoint === '') {
        alert('Please fill in the Azure Speech endpoint.');
        return;
    }

    let speechSynthesisConfig;
    if (privateEndpointEnabled) {
        speechSynthesisConfig = SpeechSDK.SpeechConfig.fromEndpoint(
            new URL(`wss://${privateEndpoint}/tts/cognitiveservices/websocket/v1?enableTalkingAvatar=true`), 
            cogSvcSubKey
        );
    } else {
        speechSynthesisConfig = SpeechSDK.SpeechConfig.fromSubscription(cogSvcSubKey, cogSvcRegion);
    }
    speechSynthesisConfig.endpointId = document.getElementById('customVoiceEndpointId').value;

    const videoFormat = new SpeechSDK.AvatarVideoFormat();
    // For demo, we use full video cropping (no cropping applied)
    videoFormat.setCropRange(new SpeechSDK.Coordinate(0, 0), new SpeechSDK.Coordinate(1920, 1080));

    const talkingAvatarCharacter = document.getElementById('talkingAvatarCharacter').value;
    const talkingAvatarStyle = document.getElementById('talkingAvatarStyle').value;
    const avatarConfig = new SpeechSDK.AvatarConfig(talkingAvatarCharacter, talkingAvatarStyle, videoFormat);
    avatarConfig.customized = document.getElementById('customizedAvatar').checked;
    avatarConfig.backgroundColor = document.getElementById('backgroundColor').value;

    avatarSynthesizer = new SpeechSDK.AvatarSynthesizer(speechSynthesisConfig, avatarConfig);
    avatarSynthesizer.avatarEventReceived = function (s, e) {
        var offsetMessage = e.offset ? ", offset: " + (e.offset / 10000) + "ms" : "";
        log("Avatar event: " + e.description + offsetMessage);
    };

    // Create a basic RTCPeerConnection if not already defined.
    if (!peerConnection) {
        peerConnection = new RTCPeerConnection({
            iceServers: [{ urls: "stun:stun.l.google.com:19302" }]
        });
    }

    // Start the avatar session using the valid peerConnection.
    avatarSynthesizer.startAvatarAsync(peerConnection).then((result) => {
        if (result.reason === SpeechSDK.ResultReason.SynthesizingAudioCompleted) {
            log("Avatar session started successfully. Result ID: " + result.resultId);
            document.getElementById('speak').disabled = false;
            document.getElementById('stopSession').disabled = false;
            document.getElementById('startSession').disabled = true;
        } else {
            log("Failed to start avatar session. Reason: " + result.reason);
        }
    }).catch((error) => {
        log("Avatar session failed to start: " + error);
    });
};

// Speak function: Uses the AvatarSynthesizer to speak SSML
window.speak = () => {
    document.getElementById('speak').disabled = true;
    document.getElementById('stopSpeaking').disabled = false;
    let spokenText = document.getElementById('spokenText').value;
    let ttsVoice = document.getElementById('ttsVoice').value;
    let personalVoiceSpeakerProfileID = document.getElementById('personalVoiceSpeakerProfileID').value;
    let spokenSsml = `<speak version='1.0' xmlns='http://www.w3.org/2001/10/synthesis' xmlns:mstts='http://www.w3.org/2001/mstts' xml:lang='en-US'>
                          <voice name='${ttsVoice}'>
                              <mstts:ttsembedding speakerProfileId='${personalVoiceSpeakerProfileID}'>
                                  <mstts:leadingsilence-exact value='0'/>
                                  ${htmlEncode(spokenText)}
                              </mstts:ttsembedding>
                          </voice>
                       </speak>`;
    log("Speak request sent at " + new Date().toISOString());
    avatarSynthesizer.speakSsmlAsync(spokenSsml).then((result) => {
        document.getElementById('speak').disabled = false;
        document.getElementById('stopSpeaking').disabled = true;
        if (result.reason === SpeechSDK.ResultReason.SynthesizingAudioCompleted) {
            log("Speech synthesized successfully. Result ID: " + result.resultId);
        } else {
            log("Unable to synthesize speech. Result ID: " + result.resultId);
            if (result.reason === SpeechSDK.ResultReason.Canceled) {
                let cancellationDetails = SpeechSDK.CancellationDetails.fromResult(result);
                log("Cancellation reason: " + cancellationDetails.reason);
                if (cancellationDetails.reason === SpeechSDK.CancellationReason.Error) {
                    log("Error details: " + cancellationDetails.errorDetails);
                }
            }
        }
    }).catch(log);
};

// Stop speaking function
window.stopSpeaking = () => {
    document.getElementById('stopSpeaking').disabled = true;
    avatarSynthesizer.stopSpeakingAsync().then(() => {
        log("Stop speaking request sent at " + new Date().toISOString());
    }).catch(log);
};

// Stop session function
window.stopSession = () => {
    document.getElementById('speak').disabled = true;
    document.getElementById('stopSession').disabled = true;
    document.getElementById('stopSpeaking').disabled = true;
    avatarSynthesizer.close();
    log("Avatar session stopped.");
    document.getElementById('startSession').disabled = false;
};

// Transparent background toggle (if needed)
window.updataTransparentBackground = () => {
    if (document.getElementById('transparentBackground').checked) {
        document.body.background = './image/background.png';
        document.getElementById('backgroundColor').value = '#00FF00FF';
        document.getElementById('backgroundColor').disabled = true;
    } else {
        document.body.background = '';
        document.getElementById('backgroundColor').value = '#FFFFFFFF';
        document.getElementById('backgroundColor').disabled = false;
    }
};

// Private endpoint toggle
window.updatePrivateEndpoint = () => {
    if (document.getElementById('enablePrivateEndpoint').checked) {
        document.getElementById('showPrivateEndpointCheckBox').hidden = false;
    } else {
        document.getElementById('showPrivateEndpointCheckBox').hidden = true;
    }
};

// Utility: HTML encode text
function htmlEncode(text) {
    const entityMap = {
        '&': '&amp;',
        '<': '&lt;',
        '>': '&gt;',
        '"': '&quot;',
        "'": '&#39;',
        '/': '&#x2F;'
    };
    return String(text).replace(/[&<>"'\/]/g, (match) => entityMap[match]);
}

// Chat streaming integration: send chat query to FastAPI backend
async function sendChatQuery() {
    const query = document.getElementById('queryInput').value.trim();
    if (!query) return;
    
    const transcriptDiv = document.getElementById('transcript');
    transcriptDiv.textContent = "Processing query...\n";
    
    try {
        const response = await fetch("/api/chat-stream", {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({ query: query })
        });
        if (!response.ok) {
            transcriptDiv.textContent += "\nError: " + response.statusText;
            return;
        }
        const reader = response.body.getReader();
        const decoder = new TextDecoder("utf-8");
        let done = false;
        let accumulatedText = "";
        while (!done) {
            const { value, done: doneReading } = await reader.read();
            done = doneReading;
            const chunk = decoder.decode(value, { stream: !done });
            const lines = chunk.split("\n");
            for (const line of lines) {
                if (line.startsWith("data: ")) {
                    const data = line.replace("data: ", "").trim();
                    if (data) {
                        accumulatedText += data + " ";
                        transcriptDiv.textContent += data + " ";
                    }
                }
            }
        }
        log("Chat response received: " + accumulatedText);
        // Optionally, you can also speak the chat response using the avatarSynthesizer:
        // avatarSynthesizer.speakSsmlAsync(/* build SSML from accumulatedText */)
    } catch (error) {
        transcriptDiv.textContent += "\nError: " + error;
        log("Error streaming chat response: " + error);
    }
}

// Bind chat query function to a button (if desired)
document.getElementById('sendQueryButton') && document.getElementById('sendQueryButton').addEventListener('click', sendChatQuery);

</script>
</body>
</html>
